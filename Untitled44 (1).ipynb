{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a2943e-de0c-4023-87cd-73d0f25f81c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Q1. What is hierarchical clustering, and how is it different from other clustering techniques?\n",
    "Hierarchical clustering is a clustering technique that builds a hierarchy of clusters either by iteratively merging smaller clusters into larger ones (agglomerative) or by splitting larger clusters into smaller ones (divisive). The main difference between hierarchical clustering and other techniques (like K-means) is that hierarchical clustering does not require the number of clusters to be specified a priori. Instead, it produces a nested sequence of clusters that can be represented as a dendrogram.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45d1169-da68-43da-820e-271e9381e5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Q2. What are the two main types of hierarchical clustering algorithms? Describe each in brief.\n",
    "Agglomerative Hierarchical Clustering:\n",
    "\n",
    "Approach: This is a bottom-up approach where each data point starts as its own cluster. Clusters are then iteratively merged based on their similarity (or distance) until all points are in one cluster or until a specified number of clusters is reached.\n",
    "Process:\n",
    "Calculate the distance between all pairs of clusters.\n",
    "Merge the two closest clusters.\n",
    "Update the distance matrix to reflect the new cluster.\n",
    "Repeat until a stopping criterion is met.\n",
    "Divisive Hierarchical Clustering:\n",
    "\n",
    "Approach: This is a top-down approach where all data points start in a single cluster. The algorithm recursively splits clusters into smaller clusters.\n",
    "Process:\n",
    "Start with a single cluster containing all data points.\n",
    "Identify the most dissimilar point or subset and split it into separate clusters.\n",
    "Repeat the process until all points are in individual clusters or a stopping criterion is met.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb144d1-2b04-4d5c-855c-fec860973a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Q3. How do you determine the distance between two clusters in hierarchical clustering, and what are the common distance metrics used?\n",
    "The distance between clusters can be determined using various linkage criteria, which define how the distance between two clusters is calculated based on the distances between their individual members. Common distance metrics include:\n",
    "\n",
    "Single Linkage: The distance between the closest points in the two clusters.\n",
    "Complete Linkage: The distance between the farthest points in the two clusters.\n",
    "Average Linkage: The average distance between all pairs of points in the two clusters.\n",
    "Ward‚Äôs Linkage: Minimizes the total within-cluster variance; it merges clusters that lead to the smallest increase in the total within-cluster variance.\n",
    "Common distance metrics for measuring similarity between individual points include:\n",
    "\n",
    "Euclidean Distance: \n",
    "ùëë\n",
    "(\n",
    "ùë•\n",
    ",\n",
    "ùë¶\n",
    ")\n",
    "=\n",
    "‚àë\n",
    "(\n",
    "ùë•\n",
    "ùëñ\n",
    "‚àí\n",
    "ùë¶\n",
    "ùëñ\n",
    ")\n",
    "2\n",
    "d(x,y)= \n",
    "‚àë(x \n",
    "i\n",
    "‚Äã\n",
    " ‚àíy \n",
    "i\n",
    "‚Äã\n",
    " ) \n",
    "2\n",
    " \n",
    "‚Äã\n",
    " \n",
    "Manhattan Distance: \n",
    "ùëë\n",
    "(\n",
    "ùë•\n",
    ",\n",
    "ùë¶\n",
    ")\n",
    "=\n",
    "‚àë\n",
    "‚à£\n",
    "ùë•\n",
    "ùëñ\n",
    "‚àí\n",
    "ùë¶\n",
    "ùëñ\n",
    "‚à£\n",
    "d(x,y)=‚àë‚à£x \n",
    "i\n",
    "‚Äã\n",
    " ‚àíy \n",
    "i\n",
    "‚Äã\n",
    " ‚à£\n",
    "Cosine Similarity: Measures the cosine of the angle between two vectors, often used for text data.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f384f4f3-cd2c-4722-9ee4-48102f3b8d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Q4. How do you determine the optimal number of clusters in hierarchical clustering, and what are some common methods used for this purpose?\n",
    "Determining the optimal number of clusters in hierarchical clustering can be done through several methods:\n",
    "\n",
    "Dendrogram Analysis: By examining the dendrogram, you can visually assess where clusters are formed. Cutting the dendrogram at a certain height corresponds to a particular number of clusters. The ideal cut is often where there is a significant vertical gap, indicating a natural division between clusters.\n",
    "\n",
    "Silhouette Score: Similar to other clustering methods, the silhouette score measures how similar an object is to its own cluster compared to other clusters. A higher silhouette score indicates better-defined clusters.\n",
    "\n",
    "Gap Statistic: This method compares the total within-cluster variation for different values of \n",
    "ùêæ\n",
    "K with the expected variation under a null reference distribution.\n",
    "\n",
    "Cluster Validity Indices: Metrics such as the Calinski-Harabasz index or the Davies-Bouldin index can also be used to assess clustering quality and help determine the optimal number of clusters.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433732aa-b47b-4a83-ab40-0601007ad37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Q5. What are dendrograms in hierarchical clustering, and how are they useful in analyzing the results?\n",
    "A dendrogram is a tree-like diagram that visually represents the arrangement of clusters in hierarchical clustering. Each leaf node represents an individual data point, while the branches represent the merging of clusters. The height of the branches indicates the distance at which clusters were merged.\n",
    "\n",
    "Usefulness:\n",
    "\n",
    "Visual Analysis: Dendrograms provide a clear visual representation of the hierarchical relationships between data points and clusters.\n",
    "Cluster Selection: By examining the dendrogram, users can make informed decisions about the number of clusters to retain based on where significant merges occur.\n",
    "Understanding Cluster Composition: They allow for exploration of how individual points contribute to different clusters.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57517b42-d6bc-4d65-85fd-825d74befc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Q6. Can hierarchical clustering be used for both numerical and categorical data? If yes, how are the distance metrics different for each type of data?\n",
    "Yes, hierarchical clustering can be applied to both numerical and categorical data, but the distance metrics used differ:\n",
    "\n",
    "Numerical Data: Common distance metrics include:\n",
    "\n",
    "Euclidean Distance: Measures the straight-line distance between points in multi-dimensional space.\n",
    "Manhattan Distance: Useful for high-dimensional data, measuring the sum of absolute differences.\n",
    "Categorical Data: Distance metrics for categorical data may include:\n",
    "\n",
    "Hamming Distance: Measures the proportion of differing attributes between two categorical points.\n",
    "Gower‚Äôs Distance: A versatile metric that can handle mixed data types (numerical and categorical) by normalizing the contribution of each type.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc6be30-06db-48d0-8fc5-3bcf66026771",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Q7. How can you use hierarchical clustering to identify outliers or anomalies in your data?\n",
    "Hierarchical clustering can help identify outliers or anomalies through the following steps:\n",
    "\n",
    "Dendrogram Analysis: When examining the dendrogram, data points that are distant from all clusters or merged at significantly higher heights than other points can be considered outliers.\n",
    "\n",
    "Cluster Size Evaluation: After forming clusters, points that belong to very small clusters (or singleton clusters) can be flagged as potential outliers.\n",
    "\n",
    "Distance Measures: Calculate the distance of points from their respective cluster centroids. Points that are far from the centroid beyond a certain threshold can be classified as anomalies.\n",
    "\n",
    "Visual Inspection: By plotting clusters, you can visually identify points that do not fit well with the other data points, further supporting the identification of outliers.\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
