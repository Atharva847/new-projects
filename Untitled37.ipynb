{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8772219a-ca4c-4bc3-85cd-e7d9925380bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Q1. What is the KNN Algorithm?\n",
    "K-Nearest Neighbors (KNN) is a simple, non-parametric, lazy learning algorithm used for classification and regression tasks. \n",
    "It classifies or predicts the output for a given data point based on the majority class (in classification) or average (in regression) \n",
    "of the K-nearest data points in the feature space.\n",
    "KNN assumes that similar data points exist close to each other.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79661d2-06aa-4a5d-8381-d404c6e68a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Q2. How Do You Choose the Value of K in KNN?\n",
    "The value of K in KNN is chosen based on the trade-off between bias and variance. Typically, K is determined using techniques like cross-validation:\n",
    "\n",
    "Small K (e.g., K=1): Low bias, high variance. The model might be sensitive to noise and can overfit.\n",
    "Large K: Higher bias, lower variance. The model might underfit, leading to oversimplified predictions.\n",
    "A commonly used approach is to test several K values and select the one that minimizes the error rate or achieves the best performance on validation data.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32df66b1-892c-4889-b1f4-cef3782e3109",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Q3. What is the Difference Between KNN Classifier and KNN Regressor?\n",
    "KNN Classifier: Used for classification tasks, where the algorithm predicts the class of a data point based on the majority vote of the K-nearest neighbors. The class with the highest vote is assigned as the prediction.\n",
    "KNN Regressor: Used for regression tasks, where the algorithm predicts a continuous value by taking the average of the K-nearest neighbors' target values.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd90e46-0301-4232-b3fa-1fa7c96e97a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Q4. How Do You Measure the Performance of KNN?\n",
    "For Classification:\n",
    "\n",
    "Accuracy: The percentage of correctly classified instances.\n",
    "Confusion Matrix: Helps calculate precision, recall, and F1-score.\n",
    "ROC-AUC: Measures the model‚Äôs ability to distinguish between classes.\n",
    "For Regression:\n",
    "\n",
    "Mean Squared Error (MSE): The average of the squared differences between the predicted and actual values.\n",
    "Mean Absolute Error (MAE): The average of the absolute differences between the predicted and actual values.\n",
    "R-squared: The proportion of the variance in the dependent variable that is predictable from the independent variables.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4435ff81-abc6-4993-8103-9dbb6cbb5d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Q5. What is the Curse of Dimensionality in KNN?\n",
    "The curse of dimensionality refers to the problem that arises when the number of features (dimensions) increases in KNN:\n",
    "\n",
    "As the number of dimensions increases, the distance between data points becomes less meaningful because all points tend to become equidistant.\n",
    "This leads to poor performance, as the algorithm struggles to identify meaningful nearest neighbors, resulting in higher computation costs and reduced model accuracy.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bd38af-1311-4aff-9d4a-229dbbfc92b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Q6. How Do You Handle Missing Values in KNN?\n",
    "Handling missing values in KNN can be done in several ways:\n",
    "\n",
    "Imputation: Replace missing values with the mean, median, or mode of the feature.\n",
    "KNN-based Imputation: Use KNN itself to predict the missing values by averaging the values of the nearest neighbors.\n",
    "Removing Data Points: In some cases, data points with missing values can be removed if the proportion of missing data is small.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3617fd49-7436-4552-a378-67267314b016",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Q7. Compare and Contrast the Performance of KNN Classifier and Regressor\n",
    "KNN Classifier:\n",
    "\n",
    "Better for: Tasks with a well-defined, categorical target variable, where the majority vote among neighbors is meaningful.\n",
    "Performance depends on: The value of K, the choice of distance metric, and the distribution of classes.\n",
    "KNN Regressor:\n",
    "\n",
    "Better for: Predicting continuous numerical values where the average of neighboring points is a good estimate.\n",
    "Performance depends on: The smoothness of the target variable in feature space, the value of K, and the choice of distance metric.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e4e9a4-409d-43c7-8726-7d82b524af4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Q8. Strengths and Weaknesses of KNN for Classification and Regression\n",
    "Strengths:\n",
    "\n",
    "Simplicity: Easy to implement and understand.\n",
    "No Training Phase: KNN is a lazy learner, so it requires no training phase.\n",
    "Adaptability: Can be used for both classification and regression tasks.\n",
    "Flexibility with Feature Types: Can handle both numerical and categorical data.\n",
    "Weaknesses:\n",
    "\n",
    "Computational Cost: High memory usage and slow prediction speed as it requires storing the entire dataset and computing distances for each prediction.\n",
    "Curse of Dimensionality: Performance deteriorates as the number of dimensions increases.\n",
    "Sensitive to Noisy Data: KNN can be easily influenced by outliers and irrelevant features.\n",
    "Addressing Weaknesses:\n",
    "\n",
    "Dimensionality Reduction: Techniques like PCA or feature selection can reduce the number of dimensions.\n",
    "Feature Scaling: Normalize or standardize features to ensure that distance calculations are meaningful.\n",
    "Efficient Data Structures: Use KD-trees or Ball trees to speed up the nearest neighbor search.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747ace57-411e-4c52-821e-049132873d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Q9. What is the Difference Between Euclidean Distance and Manhattan Distance in KNN?\n",
    "Euclidean Distance: The straight-line distance between two points in Euclidean space. It is sensitive to large differences between feature values and is more affected by outliers.\n",
    "\n",
    "ùëë\n",
    "(\n",
    "ùë•\n",
    ",\n",
    "ùë¶\n",
    ")\n",
    "=\n",
    "‚àë\n",
    "ùëñ\n",
    "=\n",
    "1\n",
    "ùëõ\n",
    "(\n",
    "ùë•\n",
    "ùëñ\n",
    "‚àí\n",
    "ùë¶\n",
    "ùëñ\n",
    ")\n",
    "2\n",
    "d(x,y)= \n",
    "i=1\n",
    "‚àë\n",
    "n\n",
    "‚Äã\n",
    " (x \n",
    "i\n",
    "‚Äã\n",
    " ‚àíy \n",
    "i\n",
    "‚Äã\n",
    " ) \n",
    "2\n",
    " \n",
    "‚Äã\n",
    " \n",
    "Manhattan Distance: The sum of the absolute differences between the coordinates of the points. It is less sensitive to outliers and works well in grid-like or high-dimensional spaces.\n",
    "\n",
    "ùëë\n",
    "(\n",
    "ùë•\n",
    ",\n",
    "ùë¶\n",
    ")\n",
    "=\n",
    "‚àë\n",
    "ùëñ\n",
    "=\n",
    "1\n",
    "ùëõ\n",
    "‚à£\n",
    "ùë•\n",
    "ùëñ\n",
    "‚àí\n",
    "ùë¶\n",
    "ùëñ\n",
    "‚à£\n",
    "d(x,y)= \n",
    "i=1\n",
    "‚àë\n",
    "n\n",
    "‚Äã\n",
    " ‚à£x \n",
    "i\n",
    "‚Äã\n",
    " ‚àíy \n",
    "i\n",
    "‚Äã\n",
    " ‚à£\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f1bc14-dbcc-4c0c-8f1b-4be9ab08ecdf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
