{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacdb50b-53d8-4644-a305-c2f257dc44be",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Q1. What is Gradient Boosting Regression?\n",
    "Gradient Boosting Regression is an ensemble technique used for regression tasks in machine learning. It builds a predictive model \n",
    "by sequentially adding weak learners, usually decision trees, and optimizes the model by minimizing a loss function using gradient descent.\n",
    "Each new learner is trained to correct the errors of the previous learners, with the objective of improving the overall performance of the\n",
    "model. The final model is a weighted sum of all the weak learners, where each learner contributes to reducing the overall prediction error.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e9fa3e-88f0-4f2c-931b-c230f3cf5712",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class GradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.models = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Initialize model predictions with the mean of y\n",
    "        y_pred = np.full(y.shape, np.mean(y))\n",
    "        \n",
    "        for _ in range(self.n_estimators):\n",
    "            # Calculate the residuals (negative gradient)\n",
    "            residuals = y - y_pred\n",
    "            \n",
    "            # Fit a weak learner to the residuals\n",
    "            model = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            model.fit(X, residuals)\n",
    "            \n",
    "            # Update predictions\n",
    "            y_pred += self.learning_rate * model.predict(X)\n",
    "            \n",
    "            # Store the model\n",
    "            self.models.append(model)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # Start with an initial prediction (mean of y in fit)\n",
    "        y_pred = np.full((X.shape[0],), np.mean(y))\n",
    "        \n",
    "        # Add predictions from each model\n",
    "        for model in self.models:\n",
    "            y_pred += self.learning_rate * model.predict(X)\n",
    "        \n",
    "        return y_pred\n",
    "\n",
    "# Example usage\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Simple dataset\n",
    "X = np.array([[1], [2], [3], [4], [5]])\n",
    "y = np.array([1.5, 2.5, 3.5, 4.5, 5.5])\n",
    "\n",
    "# Train the model\n",
    "gbr = GradientBoostingRegressor(n_estimators=10, learning_rate=0.1, max_depth=1)\n",
    "gbr.fit(X, y)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = gbr.predict(X)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "r2 = r2_score(y, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R-squared: {r2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835c0e9d-3a3b-4feb-bfa7-afb602b3dc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "# Generate a regression dataset\n",
    "X, y = make_regression(n_samples=100, n_features=1, noise=0.1)\n",
    "\n",
    "# Define the model\n",
    "gbr = GradientBoostingRegressor()\n",
    "\n",
    "# Define the grid of hyperparameters\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [1, 3, 5]\n",
    "}\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(estimator=gbr, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Get the best parameters and best score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Score (MSE):\", -grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8e0eaa-3ab4-4b16-8c4e-b5560c7bcd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Q4. What is a Weak Learner in Gradient Boosting?\n",
    "A weak learner is a model that performs slightly better than random guessing on a given task. In the context of gradient boosting, weak learners are typically simple models like decision trees with a small depth (often called decision stumps). The idea is to sequentially train these weak learners to improve the modelâ€™s predictions by correcting the errors made by previous learners.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d6a241-9025-4a94-b6ab-bddbd146482f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Q5. What is the Intuition Behind the Gradient Boosting Algorithm?\n",
    "The intuition behind gradient boosting is to iteratively reduce the residual errors made by an ensemble of weak learners. Instead of training all models independently, as in bagging, gradient boosting trains models sequentially. Each new model focuses on the errors (residuals) made by the previous ensemble, improving the accuracy with each iteration.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb97c77-c122-42f9-948e-7e913c5258c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Q6. How Does the Gradient Boosting Algorithm Build an Ensemble of Weak Learners?\n",
    "Gradient boosting builds an ensemble of weak learners by:\n",
    "\n",
    "Initial Prediction: Starting with an initial prediction, often the mean of the target variable.\n",
    "Residual Calculation: Calculating the residuals, which are the differences between the true values and the predictions.\n",
    "Weak Learner Training: Fitting a weak learner to the residuals.\n",
    "Model Update: Updating the model by adding the predictions of the weak learner, scaled by a learning rate, to the current predictions.\n",
    "Iteration: Repeating the process for a predefined number of iterations, gradually refining the predictions.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601ea569-a147-4e31-a3aa-f052414a9679",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
