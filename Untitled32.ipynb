{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaece974-d6a9-4131-8a17-988851135d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Q1. How Does Bagging Reduce Overfitting in Decision Trees?\n",
    "Bagging (Bootstrap Aggregating) reduces overfitting in decision trees by decreasing the variance of the model. Decision trees are prone \n",
    "to overfitting because they can create complex models that perfectly fit the training data, capturing noise and leading to poor \n",
    "generalization on unseen data. Bagging mitigates this by:\n",
    "\n",
    "Random Sampling: Creating multiple subsets of the training data through random sampling with replacement.\n",
    "Training Independent Models: Training a separate decision tree on each subset, which reduces the correlation between the models.\n",
    "Averaging Predictions (for Regression) or Majority Voting (for Classification): The final prediction is obtained by averaging the outputs \n",
    "(in regression) or taking the majority vote (in classification) across all the trees, which smooths out the individual tree's predictions\n",
    "and reduces the risk of overfitting.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c8f48b-16bd-44e5-bd3a-02be80958b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Q2. What Are the Advantages and Disadvantages of Using Different Types of Base Learners in Bagging?\n",
    "Advantages:\n",
    "\n",
    "Weak Learners (e.g., Decision Stumps): Using simple models as base learners can lead to better generalization and quicker training. They benefit significantly from the ensemble effect, where their individual weaknesses are compensated by the ensemble.\n",
    "Strong Learners (e.g., Deep Decision Trees): These can capture more complex patterns, and when combined in an ensemble, they can produce highly accurate models.\n",
    "Disadvantages:\n",
    "\n",
    "Weak Learners: While they reduce variance, they may still introduce bias if the individual models are too simple to capture the underlying data patterns.\n",
    "Strong Learners: These can be computationally expensive and might still overfit the training data, although bagging typically reduces this risk. They might also increase the ensembleâ€™s variance if not enough diversity is achieved among the models.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb195338-a8ef-473b-ae7b-00b3c10740dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Q3. How Does the Choice of Base Learner Affect the Bias-Variance Tradeoff in Bagging?\n",
    "High-Bias Learners (e.g., Shallow Trees): Bagging can help reduce variance but does not address the bias. The ensemble might still be biased if the individual models are too simplistic, leading to underfitting.\n",
    "High-Variance Learners (e.g., Deep Trees): Bagging is particularly effective here, as it reduces the variance by averaging multiple high-variance models. This results in a model that maintains low bias while reducing the overfitting tendency.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48e254a-1099-4a2b-8c5c-338e367458e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Q4. Can Bagging Be Used for Both Classification and Regression Tasks? How Does It Differ in Each Case?\n",
    "Yes, bagging can be used for both classification and regression tasks.\n",
    "\n",
    "Classification:\n",
    "Voting: The predictions from each model in the ensemble are combined using majority voting. The class with the most votes across all models is chosen as the final prediction.\n",
    "Regression:\n",
    "Averaging: The predictions from each model are averaged to produce the final output. This averaging helps to smooth out the predictions, reducing variance.\n",
    "In both cases, bagging helps to create more stable and generalizable models, but the method of combining the outputs differs depending on whether the task is classification or regression.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f43c489-9001-4f1e-a6c5-e88b984755f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Q5. What is the Role of Ensemble Size in Bagging? How Many Models Should Be Included in the Ensemble?\n",
    "Role of Ensemble Size:\n",
    "\n",
    "The ensemble size (the number of models included in the ensemble) directly affects the performance of bagging. Larger ensembles generally lead to more stable and accurate predictions because they reduce the overall variance of the model.\n",
    "How Many Models Should Be Included:\n",
    "\n",
    "There is no fixed number, but typically, the performance improves as more models are added, up to a point. After a certain number of models, the performance gains diminish (law of diminishing returns), and additional models may not significantly improve accuracy but will increase computational cost.\n",
    "In practice, the ensemble size is chosen based on a trade-off between computational resources and desired performance. Commonly used ensemble sizes range from 50 to 500 models, depending on the complexity of the data and the computational budget.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd6a84a-cc55-4bb0-b4ac-215e155a06f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Q6. Example of a Real-World Application of Bagging in Machine Learning\n",
    "Real-World Application: Credit Scoring\n",
    "\n",
    "Problem: Predicting whether a loan applicant will default on a loan.\n",
    "Bagging in Use: A bank or financial institution could use bagging with decision trees to build a robust credit scoring model. Each tree \n",
    "in the ensemble might be trained on a different bootstrap sample of the customer data, which includes features like income, credit \n",
    "history, employment status, etc.\n",
    "Outcome: The bagging approach helps to create a stable model that reduces the risk of overfitting, leading to more reliable predictions \n",
    "about whether a customer will default. This can be crucial in minimizing financial risk for the institution.\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
